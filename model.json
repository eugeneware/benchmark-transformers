{
  "format": "torch",
  "nodes": [
    {
      "name": "distilbert",
      "id": 140291762745200,
      "class_name": "DistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (1): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (2): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (3): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (4): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (5): TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n)",
      "parameters": [
        [
          "embeddings.word_embeddings.weight",
          [
            28996,
            768
          ]
        ],
        [
          "embeddings.position_embeddings.weight",
          [
            512,
            768
          ]
        ],
        [
          "embeddings.LayerNorm.weight",
          [
            768
          ]
        ],
        [
          "embeddings.LayerNorm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.0.attention.q_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.0.attention.q_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.0.attention.k_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.0.attention.k_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.0.attention.v_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.0.attention.v_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.0.attention.out_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.0.attention.out_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.0.sa_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.0.sa_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.0.ffn.lin1.weight",
          [
            3072,
            768
          ]
        ],
        [
          "transformer.layer.0.ffn.lin1.bias",
          [
            3072
          ]
        ],
        [
          "transformer.layer.0.ffn.lin2.weight",
          [
            768,
            3072
          ]
        ],
        [
          "transformer.layer.0.ffn.lin2.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.0.output_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.0.output_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.1.attention.q_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.1.attention.q_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.1.attention.k_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.1.attention.k_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.1.attention.v_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.1.attention.v_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.1.attention.out_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.1.attention.out_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.1.sa_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.1.sa_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.1.ffn.lin1.weight",
          [
            3072,
            768
          ]
        ],
        [
          "transformer.layer.1.ffn.lin1.bias",
          [
            3072
          ]
        ],
        [
          "transformer.layer.1.ffn.lin2.weight",
          [
            768,
            3072
          ]
        ],
        [
          "transformer.layer.1.ffn.lin2.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.1.output_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.1.output_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.2.attention.q_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.2.attention.q_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.2.attention.k_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.2.attention.k_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.2.attention.v_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.2.attention.v_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.2.attention.out_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.2.attention.out_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.2.sa_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.2.sa_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.2.ffn.lin1.weight",
          [
            3072,
            768
          ]
        ],
        [
          "transformer.layer.2.ffn.lin1.bias",
          [
            3072
          ]
        ],
        [
          "transformer.layer.2.ffn.lin2.weight",
          [
            768,
            3072
          ]
        ],
        [
          "transformer.layer.2.ffn.lin2.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.2.output_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.2.output_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.3.attention.q_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.3.attention.q_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.3.attention.k_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.3.attention.k_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.3.attention.v_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.3.attention.v_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.3.attention.out_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.3.attention.out_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.3.sa_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.3.sa_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.3.ffn.lin1.weight",
          [
            3072,
            768
          ]
        ],
        [
          "transformer.layer.3.ffn.lin1.bias",
          [
            3072
          ]
        ],
        [
          "transformer.layer.3.ffn.lin2.weight",
          [
            768,
            3072
          ]
        ],
        [
          "transformer.layer.3.ffn.lin2.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.3.output_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.3.output_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.4.attention.q_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.4.attention.q_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.4.attention.k_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.4.attention.k_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.4.attention.v_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.4.attention.v_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.4.attention.out_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.4.attention.out_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.4.sa_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.4.sa_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.4.ffn.lin1.weight",
          [
            3072,
            768
          ]
        ],
        [
          "transformer.layer.4.ffn.lin1.bias",
          [
            3072
          ]
        ],
        [
          "transformer.layer.4.ffn.lin2.weight",
          [
            768,
            3072
          ]
        ],
        [
          "transformer.layer.4.ffn.lin2.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.4.output_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.4.output_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.5.attention.q_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.5.attention.q_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.5.attention.k_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.5.attention.k_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.5.attention.v_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.5.attention.v_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.5.attention.out_lin.weight",
          [
            768,
            768
          ]
        ],
        [
          "transformer.layer.5.attention.out_lin.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.5.sa_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.5.sa_layer_norm.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.5.ffn.lin1.weight",
          [
            3072,
            768
          ]
        ],
        [
          "transformer.layer.5.ffn.lin1.bias",
          [
            3072
          ]
        ],
        [
          "transformer.layer.5.ffn.lin2.weight",
          [
            768,
            3072
          ]
        ],
        [
          "transformer.layer.5.ffn.lin2.bias",
          [
            768
          ]
        ],
        [
          "transformer.layer.5.output_layer_norm.weight",
          [
            768
          ]
        ],
        [
          "transformer.layer.5.output_layer_norm.bias",
          [
            768
          ]
        ]
      ],
      "output_shape": [
        [
          32,
          128,
          768
        ]
      ],
      "num_parameters": [
        22268928,
        393216,
        768,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        768,
        768,
        2359296,
        3072,
        2359296,
        768,
        768,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        768,
        768,
        2359296,
        3072,
        2359296,
        768,
        768,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        768,
        768,
        2359296,
        3072,
        2359296,
        768,
        768,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        768,
        768,
        2359296,
        3072,
        2359296,
        768,
        768,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        768,
        768,
        2359296,
        3072,
        2359296,
        768,
        768,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        589824,
        768,
        768,
        768,
        2359296,
        3072,
        2359296,
        768,
        768,
        768
      ]
    },
    {
      "name": "pre_classifier",
      "id": 140291762745248,
      "class_name": "Linear(in_features=768, out_features=768, bias=True)",
      "parameters": [
        [
          "weight",
          [
            768,
            768
          ]
        ],
        [
          "bias",
          [
            768
          ]
        ]
      ],
      "output_shape": [
        [
          32,
          768
        ]
      ],
      "num_parameters": [
        589824,
        768
      ]
    },
    {
      "name": "dropout",
      "id": 140291762744144,
      "class_name": "Dropout(p=0.2, inplace=False)",
      "parameters": [],
      "output_shape": [
        [
          32,
          768
        ]
      ],
      "num_parameters": []
    },
    {
      "name": "classifier",
      "id": 140291762745296,
      "class_name": "Linear(in_features=768, out_features=3, bias=True)",
      "parameters": [
        [
          "weight",
          [
            3,
            768
          ]
        ],
        [
          "bias",
          [
            3
          ]
        ]
      ],
      "output_shape": [
        [
          32,
          3
        ]
      ],
      "num_parameters": [
        2304,
        3
      ]
    }
  ],
  "edges": []
}
